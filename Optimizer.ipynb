{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfc6282",
   "metadata": {},
   "source": [
    "# Optimizers implememtation(Manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf437c8",
   "metadata": {},
   "source": [
    "# Basic Setup (Neural Network Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cd9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a simple synthetic dataset\n",
    "np.random.seed(1)\n",
    "m = 1000 # Number of examples\n",
    "n_x = 10  # Number of input features\n",
    "\n",
    "X = np.random.randn(n_x, m)\n",
    "Y = (np.sum(X, axis=0) > 0).astype(int).reshape(1, m)  # Simple rule-based labels\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.2, random_state=1)\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "Y_train, Y_test = Y_train.T, Y_test.T\n",
    "\n",
    "\n",
    "# Initialize parameters (weights)\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Forward Propagation (simple linear model for illustration)\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    return A1\n",
    "\n",
    "# Loss function (Mean Squared Error)\n",
    "def compute_loss(A1, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = (1/m) * np.sum((A1 - Y)**2)\n",
    "    return loss\n",
    "\n",
    "# Backward Propagation (simple linear model)\n",
    "def backward_propagation(X, Y, A1, parameters):\n",
    "    m = X.shape[1]\n",
    "    dZ1 = A1 - Y\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {'dW1': dW1, 'db1': db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceeeb22",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ba8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(parameters, grads, learning_rate):\n",
    "    W1 = parameters['W1'] - learning_rate * grads['dW1']\n",
    "    b1 = parameters['b1'] - learning_rate * grads['db1']\n",
    "\n",
    "    parameters['W1'] = W1\n",
    "    parameters['b1'] = b1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90424339",
   "metadata": {},
   "source": [
    "# 2. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d40d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(parameters, grads, learning_rate):\n",
    "    W1 = parameters['W1'] - learning_rate * grads['dW1']\n",
    "    b1 = parameters['b1'] - learning_rate * grads['db1']\n",
    "\n",
    "    parameters['W1'] = W1\n",
    "    parameters['b1'] = b1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd96624",
   "metadata": {},
   "source": [
    "# 3. SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5694bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(parameters, grads, velocity, beta, learning_rate):\n",
    "    velocity['dW1'] = beta * velocity['dW1'] + (1 - beta) * grads['dW1']\n",
    "    velocity['db1'] = beta * velocity['db1'] + (1 - beta) * grads['db1']\n",
    "    \n",
    "    parameters['W1'] = parameters['W1'] - learning_rate * velocity['dW1']\n",
    "    parameters['b1'] = parameters['b1'] - learning_rate * velocity['db1']\n",
    "    \n",
    "    return parameters, velocity\n",
    "\n",
    "# Initialize velocity for momentum\n",
    "def initialize_velocity(parameters):\n",
    "    velocity = {}\n",
    "    velocity['dW1'] = np.zeros_like(parameters['W1'])\n",
    "    velocity['db1'] = np.zeros_like(parameters['b1'])\n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c879b6",
   "metadata": {},
   "source": [
    "# 4. RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d742efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_update(parameters, grads, s, beta2, epsilon, learning_rate):\n",
    "    s['dW1'] = beta2 * s['dW1'] + (1 - beta2) * np.square(grads['dW1'])\n",
    "    s['db1'] = beta2 * s['db1'] + (1 - beta2) * np.square(grads['db1'])\n",
    "    \n",
    "    parameters['W1'] -= learning_rate * grads['dW1'] / (np.sqrt(s['dW1']) + epsilon)\n",
    "    parameters['b1'] -= learning_rate * grads['db1'] / (np.sqrt(s['db1']) + epsilon)\n",
    "    \n",
    "    return parameters, s\n",
    "\n",
    "# Initialize the RMSProp cache\n",
    "def initialize_rmsprop(parameters):\n",
    "    s = {}\n",
    "    s['dW1'] = np.zeros_like(parameters['W1'])\n",
    "    s['db1'] = np.zeros_like(parameters['b1'])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72bafe",
   "metadata": {},
   "source": [
    "# 5. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a938d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(parameters, grads, velocity, s, t, beta1, beta2, epsilon, learning_rate):\n",
    "    # Momentum update\n",
    "    velocity['dW1'] = beta1 * velocity['dW1'] + (1 - beta1) * grads['dW1']\n",
    "    velocity['db1'] = beta1 * velocity['db1'] + (1 - beta1) * grads['db1']\n",
    "    \n",
    "    # RMSProp update\n",
    "    s['dW1'] = beta2 * s['dW1'] + (1 - beta2) * np.square(grads['dW1'])\n",
    "    s['db1'] = beta2 * s['db1'] + (1 - beta2) * np.square(grads['db1'])\n",
    "    \n",
    "    # Bias correction\n",
    "    v_corrected_dW1 = velocity['dW1'] / (1 - beta1 ** t)\n",
    "    v_corrected_db1 = velocity['db1'] / (1 - beta1 ** t)\n",
    "    s_corrected_dW1 = s['dW1'] / (1 - beta2 ** t)\n",
    "    s_corrected_db1 = s['db1'] / (1 - beta2 ** t)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters['W1'] -= learning_rate * v_corrected_dW1 / (np.sqrt(s_corrected_dW1) + epsilon)\n",
    "    parameters['b1'] -= learning_rate * v_corrected_db1 / (np.sqrt(s_corrected_db1) + epsilon)\n",
    "    \n",
    "    return parameters, velocity, s\n",
    "\n",
    "# Initialize for Adam optimizer\n",
    "def initialize_adam(parameters):\n",
    "    velocity = initialize_velocity(parameters)\n",
    "    s = initialize_rmsprop(parameters)\n",
    "    return velocity, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ccc47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with gd optimizer: 0.9\n",
      "Accuracy with sgd optimizer: 0.9\n",
      "Accuracy with momentum optimizer: 0.9\n",
      "Accuracy with rmsprop optimizer: 0.865\n",
      "Accuracy with adam optimizer: 0.9\n"
     ]
    }
   ],
   "source": [
    "def train_model(optimizer, X_train, Y_train, X_test, Y_test, num_iterations=1000, learning_rate=0.01):\n",
    "    layer_dims = [X_train.shape[0], 1]\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    if optimizer == 'momentum':\n",
    "        velocity = initialize_velocity(parameters)\n",
    "        beta = 0.9\n",
    "    elif optimizer == 'rmsprop':\n",
    "        s = initialize_rmsprop(parameters)\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-8\n",
    "    elif optimizer == 'adam':\n",
    "        velocity, s = initialize_adam(parameters)\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-8\n",
    "    \n",
    "    for t in range(1, num_iterations + 1):\n",
    "        A1 = forward_propagation(X_train, parameters)\n",
    "        loss = compute_loss(A1, Y_train)\n",
    "        grads = backward_propagation(X_train, Y_train, A1, parameters)\n",
    "\n",
    "        if optimizer == 'gd':\n",
    "            parameters = gradient_descent_update(parameters, grads, learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            parameters = sgd_update(parameters, grads, learning_rate)\n",
    "        elif optimizer == 'momentum':\n",
    "            parameters, velocity = momentum_update(parameters, grads, velocity, beta, learning_rate)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            parameters, s = rmsprop_update(parameters, grads, s, beta2, epsilon, learning_rate)\n",
    "        elif optimizer == 'adam':\n",
    "            parameters, velocity, s = adam_update(parameters, grads, velocity, s, t, beta1, beta2, epsilon, learning_rate)\n",
    "\n",
    "    # Predict on test set\n",
    "    A1_test = forward_propagation(X_test, parameters)\n",
    "    predictions = (A1_test > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(Y_test.T, predictions.T)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Train models using different optimizers\n",
    "optimizers = ['gd', 'sgd', 'momentum', 'rmsprop', 'adam']\n",
    "for opt in optimizers:\n",
    "    accuracy = train_model(opt, X_train, Y_train, X_test, Y_test, num_iterations=1000, learning_rate=0.1)\n",
    "    print(f\"Accuracy with {opt} optimizer: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
